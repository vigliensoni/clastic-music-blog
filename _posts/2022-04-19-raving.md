---
layout: post
current: post
cover:  assets/images/RAVE/rave.png
navigation: True
title: RAVE modelling
date: 2022-04-10 5:34:00
tags: [RAVE]
class: post-template
subclass: 'post'
author: gabriel
---

### RAVE
- Training RAVE consists of two steps:
  - RAVE training stages
    - **representational learning**: VAE training 
    - **adversarial fine tuning**: encoder is fixed, and the decoder is trained with a GAN. 
  - PRIOR training
    - **latent representation compactness**

#### Training

Last week I finished the full training of a model. The dataset I used consists of a series of recordings and interviews collected by the Aventures Sonores collective. The dataset consists of 4.5 hours of speech recordings of:
  - Poetry read by friends
  - Conversations and historical recording s with First Nations from South America
  - Audio documents about space and time

For this run, RAVE was trained with all default values: number of steps, fidelity, automatic dimensionality reduction, capacity, network architecture, etc.

The next figure shows the number of steps against epochs. It can be seen that the PRIOR training stopped at 100,000 epochs (default value) and took 1d 18h. RAVE training part (first two steps) stopped at 2M steps and took 4d 10h.

![prior-training-01](/assets/images/RAVE/rave-prior-training-01.png)

With the default values, RAVE is trained for 2M steps. The VAE part is trained for 1M steps and then the GAN stage starts automatically. The figure shows *distance* metric against step number.

![training-01](/assets/images/RAVE/rave-training-01.png)


#### Resulting model in musical applications

I tested the model with the `nn~` MaxMSP object. I instantiated as a MaxMSP standalone and as a M4L patch inside Ableton. Both configurations allowed me to prime or excite the model decoder with actual audio recordings, while at the same time exploring the latent space of the decoder in real time. 

Here are two audio examples of the first experimentations:

 <audio controls>
  <source src="assets/sounds/AS-01.mp3" type="audio/mpeg">
Your browser does not support the audio element.
</audio> 

- The previous example has a few glitches due to computer processing. I used a better computer in the second run and adjusted the buffer of the `nn~` object in order to reduce artifacts,

<audio controls>
  <source src="assets/sounds/AS-02.mp3" type="audio/mpeg">
Your browser does not support the audio element.
</audio> 


#### Training more models

I am now training seven new models consisting of the following corpora:
- Acoustic harmonic (33h of audio files)
- Acoustic percussive (17h)
- Electronic harmonic (16h)
- Electronic percussive (15h)
- Caterina Barbieri (6h)
- Vigliensoni (6h)

I run these training with the following changes:
  - KL divergence is now fixed (not variable)
  - Number of max steps was now set to 3M steps ([MAX_STEPS = 3000000](https://github.com/acids-ircam/RAVE/blob/b9e486570bfad18144f5b975734e9618967e1290/train_rave.py#L51)), to match the experiments of the paper. This max step number is not evenly distributed between VAE and GAN, but is 1M for VAE, and the rest for the GAN part. The VAE part of the training seems to be linked to a `warmup` parameter in the repo ([WARMUP = 1000000](https://github.com/acids-ircam/RAVE/blob/b9e486570bfad18144f5b975734e9618967e1290/train_rave.py#L43))
  
These models are currently being trained. As shown in the figure, the adversarial fine tuning started at 2M steps.


![training-02](/assets/images/RAVE/rave-training-02.png)

### Data augmentation

RAVE provides a command line method for data augmentation

```
resample --sr 48000 --augment
resample --sr 48000 --input 06-VIGLIENSONI-AUG --output 06-VIGLIENSONI-AUG/out_48000 --augment
```

The method converts all audio file types to wav, converts them to a mono file, and resamples them to the desired sample rate. 

It also comes with a method for measuring the duration of all audio files in a folder

```
duration
```

I applied data augmentation to the AS dataset, which consists mostly in spoken voice, and got the following results:

```
cd the-original-folder
duration

found 4 .wav files
total duration: 0h 12m 52s

cd the-augmented-folder
duration

Found 814 .wav files
total duration: 10h 31m 12s
```

I checked and listened to the results of the augmentation and the dynamics, sr, and filetype of the files were actually changed.

#### New jobs

- [ ] I submitted two new jobs with the augmented data.  I made a change to the training parameters. The first one was to use the flag `--cropped-latent-size 16` to see if the dimensionality of the resulting model is fixed instead of being computed automatically reduced. The target datasets for these new trainings are 
  - AS
  - VIGLIENSONI



#### QUESTIONS 

What are the following terms:
```
capacity : capability to learn some arbitrary function which can map the data to an index. there's capacity for the encoder and the decoder.
no-latency : related to PQMF (pseudo-quadrature mirror filters), sub-band coding for neural vocoders
cropped-latent-size : is this related to setting up a fixed latent space size instead of learning one?
```

