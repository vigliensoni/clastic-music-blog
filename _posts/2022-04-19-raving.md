---
layout: post
current: post
cover:  assets/images/rave.png
navigation: True
title: RAVE modelling
date: 2022-04-10 5:34:00
tags: [RAVE]
class: post-template
subclass: 'post'
author: gabriel
---

### RAVE
- Training consists of two steps, and the first training step has two stages:
  - Representational learning: VAE training 
  - Adversarial fine tuning: 
  - Latent representation compactness: PRIOR training

- Finished a full model creation last week. 
- The dataset consists of a series of recordings and interviews that the people I'm working with in Mallorca use for their events (4.5 hours)
  - Poetry read by friends
  - Conversations and historical recording s with First Nations from South America
  - Audio documents about space and time
  
- With the default values of the repo (i.e., number of steps, latent space dimensions, fidelity) 
  - RAVE took 4.5 days to be trained (1st: 1d10h, 3nd: 3d)
  - PRIOR took 1 day to be trained

- Tested the model with the MaxMSP object, was able to use it inside M4L and prime the model with audio coming from the DAW, while at the same time exploring the latent spaces.

 <audio controls>
  <source src="assets/sounds/AS-01.mp3" type="audio/mpeg">
Your browser does not support the audio element.
</audio> 

- A few glitches here and there due to computer processing. Much better in second try. 

<audio controls>
  <source src="assets/sounds/AS-02.mp3" type="audio/mpeg">
Your browser does not support the audio element.
</audio> 


- **New training**
  - 7 models, music files of qualities
    - Acoustic harmonic (33h)
    - Acoustic percussive (17h)
    - Electronic harmonic (16h)
    - Electronic percussive (15h)
    - Caterina Barbieri (6h)
    - Vigliensoni (6h)
  - Some changes with the previous full training
    - KL divergence now fixed (not variable)
    - Number of max steps now match the experiments of the paper (i.e., 3M steps)





